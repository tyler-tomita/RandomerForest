\documentclass[10pt]{article}
\input{preamble.tex}


\title{\vspace{-50pt}\db{Supervised Manifold Learning for Wide Data}}
\author{Joshua T.~Vogelstein, Mauro Maggioni}
\date{}

\begin{document}
\maketitle

\begin{abstract}
In the 21st century high-dimensional observations abound.  In classical statistics, it is well known that as the dimensionality of a dataset increase, the number of samples required to estimate properties thereof increases even faster.  A number of techniques have been developed to stave off this ``curse of dimensionality'', including dimensionality reduction and regularization.  Mitigating this curse is especially demanding in supervised learning problems, such as classification. The challenge in finding a low-dimensional discriminant boundary in the face of high-dimensional observations is that direct searches are computationally infeasible.  Therefore, previous art have taken one of two approaches. First, one can ignore the discriminant problem, and try to reduce the dimensionality of the data---for example, using principal components analysis or manifold learning techniques---in hopes that one has not discarded the discriminant information.  Second, one directly solve the problem after making a number of overly restrictive assumptions of the data, such as independence and sparsity.  Here, we demonstrate via a simple geometric argument that one can algebraically compute a low-dimensional embedding that is approximately optimal with high probability in many settings.  The intuition is that we can  project the data both in the directions of maximal variance (ignoring the classification task) and the linear direction of maximal discrimination (ignoring the variance).  The result is an extremely computationally efficient supervised manifold learning method that we call ``Linear Optimal Low-Rank'' (LOL). LOL outperforms state-of-the-art algorithms in a wide variety of settings, including both those for which theoretical arguments predict it should, and in much more general settings for which we are unable as yet to obtain solid theoretical footing.  This includes both simulated settings, where our intuition enables us to characterize a number of simple variants designed for more complex settings, as well as several benchmark problems.  In particular, LOL outperforms several reference algorithms in terms of both efficiency and accuracy on said benchmarks. We therefore believe that LOL could be useful in myriad applications, and many extensions are readily available.
\end{abstract}

\vspace{15pt}


Supervised learning (SL)---the art and science of discovering statistical relationships between data of multiple different measurement types---is one of the most useful tools in the scientific toolbox.  SL has been enabled a wide variety of basic and applied findings, ranging from discovering biomarkers in omics data \cite{XXX}, to object recognition from images \cite{XXX}, and includes celebrated methods such as support vector machines, random forests, and deep networks \cite{XXX}.  A special case of SL is classification; a classifier can predict the 'class' of a novel observation via training on a set of paired observations and class labels (for example, predicting male vs. female from MRI scans).   In such problems, the goal is to find the optimal discriminant boundary, which partitions the space of observations into the different classes.  In this big data age, the ambient (or observed) dimensionality of the observations is quickly ballooning, and with it, the ambient dimensionality of the discriminant boundary.  While historical data may have consisted of only a few dimensions (e.g., height and weight), modern scientific datasets often consist of hundreds, thousands, or even millions of dimensions (e.g. genetics, neuroscience, omics). Regardless of the dimensionality, when a scientist or analyst obtains a new dataset consisting of some observations and labels, she must decide which of the myriad available tools to use. {Reference} algorithms for datasets with low dimensionality include linear and quadratic discriminant analysis, support vector machines, and random forests \cite{Hastie2004}. 


Classical methods, however, often rely on very restrictive assumptions. In particular, the theoretical guarantees upon which many classical methods rest require that the number of samples is much larger than the dimensionality of the problem ($n \gg p)$.  
In scientific contexts, while the dimensionality of datasets is booming, the sample is not witnessing a concomitant increase (see, for example, Table 2 of \cite{diMartino2013a} in connectomics).
When the number of dimensions is orders of magnitude larger than the sample size, as is now typical, this $n \gg p$ assumption is woefully inadequate.
This inadequacy is not a mere theoretical footnote; rather, the implementation of the algorithm itself sometimes fails or crashes if this assumption is not met.  Worse, often times the algorithm will run to completion, but the answer will be essentially random, with little or no predictive power or accuracy (e.g., \cite{Eklund2012}).

To combat these issues, the fields of statistics and machine learning have developed a large collection of new methods that relax these assumptions, and exhibit significantly improved performance characteristics.  Each such approach makes some ``structural'' assumptions about the data (sometimes in the form of priors), therefore potentially adding some bias, while reducing variance.  The best approach, for a given problem, is the approach that wins this bias/variance trade-off; that is, the approach whose assumptions best correspond to the properties of the data (minimizing bias), and yields estimates have that low error under those assumptions (minimizing variance). These approaches complement ``feature engineering'' approaches, where the practitioner designs new features based on prior domain specific knowledge.



One of the earliest approaches for discovering low-dimensional representations in supervised learning problems is regularization or shrinkage \cite{Friedman1989a,Bickel2004a,Bouveyron2007}.  Many shrinkage methods  mitigate the dimensionality problem by smoothing \cite{Witten2009a}, for example, by regressing parameters to the mean.  More recently, a special case of regularization has risen to prominence, called \emph{sparsity} \cite{Olshausen1997a}, in which it is assumed that a small number of dimensions can encode the discriminant boundary \cite{Tibshirani1996}.  This assumption, when accurate, can lead to substantial improvements in accuracy with relatively moderate increase in computational cost \cite{Efron2004}.  This framework includes methods such as \sct{Lasso}~\cite{Tibshirani1996}, higher criticism thresholding \cite{Donoho2008a}, sure independence screening \cite{Fan2008}, distance correlation SIS \cite{Li2012},  and sparse variants of linear discriminant analysis \cite{Tibshirani2002a,Fan2008a,Witten2009a,Clemmensen2011a,Mai2013a,Fan2012a}.  




However, for a wide class of problems, such as image classification, sparisty in the ambient space is an overly restrictive, and therefore, bias-inducing assumption (see Figure \ref{f:mnist} for an example on the classic MNIST dataset). A generalization of sparsity is ``low-rank'', in which a  small number of linear combinations of the ambient dimensions characterize the data (which can be thought of as sparsity in the eigenspace of the data).  Unsupervised low-rank methods date back over a century, including multidimensional scaling \cite{Householder1938,Borg2005} and principal components analysis \cite{Pearson1901a,Jolliffe2002a}. More recent nonlinear versions of unsupervised dimensionality reduction, or manifold learning, include developments from neural network theory such as self-organizing maps \cite{Kohonen1982a}, generative topographic mapping \cite{Bishop1998a}. In this century, manifold learning became more popular, including isomap \cite{Tenenbaum2000a}, local linear embedding \cite{Roweis2000a}, Laplacian eigenmaps \cite{Belkin2003a}, local tangent space alignment \cite{Zhang2004f}, diffusion maps \cite{Coifman2006a},  and geometric multi-resolution analysis \cite{Allard2012}.  All these approaches can be used as pre-processing steps, to reduce the dimensionality of the data prior to solving the supervised learning problem \cite{Belhumeur1997a}.

However, such manifold learning methods, while exhibiting both strong theoretical \cite{Eckart1936a,deSilva2003, Allard2012} and empirical performance, are fully unsupervised.  Thus, in classification problems, they discover a low-dimensional representation of the data, ignoring the labels.  This can be highly problematic when the discriminant dimensions and the directions of maximal variance in the learned manifold are not aligned (see Figure \ref{f:mnist} for an example).  Supervised dimensionality reduction techniques, therefore, combine the best of both worlds, explicitly searching for low-dimensional discriminant boundaries.  A set of methods from the statistics community is collectively referred to as   ``sufficient dimensionality reduction'' (SIR) or ``first two moments'' (F2M) methods  \cite{Li1991a, Tishby1999a, Globerson2003a, Cook2005a,Fukumizu2004a}.  These methods are theoretically elegant, but typically require the sample size to be larger than the number of observed dimensions (although see \cite{Cook2013} for some promising work).  Other approaches formulate an optimization problem, such as projection pursuit \cite{Huber1985a}, empirical risk minimization \cite{Belkin2006a}, or supervised dictionary learning \cite{Mairal2009}.  These methods are limited because they are prone to fall into local minima, they require costly iterative algorithms, and lack any theoretical guarantees \cite{Belkin2006a}.   Thus, there remains a gap in the literature: a supervised learning method with theoretical convergence guarantees appropriate when the dimensionality is orders of magnitude larger than the sample size.


The challenge lies is posing the problem in such a way that efficient numerical algorithms can be brought to bear, without costly iterations or tuning parameters.  Our approach, which we call ``Linear Optimal Low-rank'' (\Lol) embedding (see Figure \ref{f:mnist}), utilizes the first two moments, as do SIR, spectral decompositions, and high-dimensional discriminant analysis methods \cite{Bouveyron2007}, but does not require iterative algorithms and therefore is vastly more computationally efficient.  The motivation for \Lol~comes from a simple geometric intuition (Figure \ref{f:cigars}).  Indeed, we provide  theoretical insight explaining why our method is more general than previous approaches.  A variety of simulations provide further evidence that \Lol~efficiently finds a better low-dimensional representation than competing methods, not just under the provable model assumptions, but also under much more general contexts (Figure \ref{f:properties}).  Moreover, we demonstrate that \Lol~achieves better performance, in less time, as compared to several reference high-dimensional classifiers, on several benchmark datasets, including genomics, connectomics, and image processing problems (Figure \ref{f:realdata}). 
Finally, \Lol~can also be used to improve high-dimensional regression and testing (Figure \ref{f:generalizations}).  
Based on the above, we hope that  \Lol~can be useful for many modern supervised learning problems, as well as be extended for even further generality. For reproducibility and extensibility, we provide basic R code, and detailed MATLAB code to run all numerical experiments and reproduce all figures is available from our github repository available here: \url{http://docs.neurodata.io/lol}. 


\section*{Results}

\subsection*{An Illustrative Real Data Example of Supervised Linear Manifold Learning}



% \begin{wrapfigure}{R}{0.7\textwidth}%[h!]
\begin{figure}
\centering % l b r t
\includegraphics[width=0.8\linewidth,trim=1cm 0cm 0cm 4.0cm,clip=true]{../Figs/mnist}
\caption{
Illustrating three different classifiers---\sct{Lasso}~(top), \sct{Fld$\circ$Pca}~(middle), and \Lol~(bottom)---for embedding images of the digits 3, 7, and 8 (from MNIST), each of which is 28 $\times$ 28 = 784 dimensional.  
\textbf{(A)}: Exemplars, boundary colors are only for visualization purposes.
\textbf{(B)}: The first four projection matrices learned by the three different approaches on 300 training samples. Note that \sct{Lasso}~is sparse and supervised, \Pca~is dense and unsupervised, and \Lol~is dense and supervised.
\textbf{(C)}: Embedding 500 test samples into the top 2 dimensions using each approach.  Digits color coded as in (A).
\textbf{(D)}:  The estimated posterior distribution of test samples after 
5-dimensional projection learned via each method.  
We show only 3 vs. 8 for simplicity.
The vertical line shows the classification threshold.
The filled area is the estimated error rate: the goal of any classification algorithm is to minimize that area. 
Clearly, \Lol~exhibits the best separation after embedding, which results in the best classification performance.
}
\label{f:mnist}
% \end{wrapfigure}
\end{figure}

Pseudocode of any method that embeds high-dimensional data as part of classification proceeds as schematized in Figure \ref{f:mnist}: 
(A) obtain/select $n$ training samples of the data, 
(B) learn a low dimensional projection, 
(C) project $m$ testing samples onto the lower dimensional space, 
(D) classify the embedded testing samples using some classifier.  
We consider  three different linear dimensionality reduction methods---\sct{Lasso}, \Pca, and \Lol---each of which we compose with a classifier to form high-dimensional classifiers.\footnote{Although \sct{Lasso}~is not a 2-step method (where embedding is learned first, and then a classifier is applied), adaptive lasso \cite{Zou2006a} and its variants improve on lasso's theoretical and empirical properties, so we consider such an approach here.}

To demonstrate the utility of \Lol, we  first consider one of the most popular benchmark datasets ever, the MNIST dataset \cite{mnist}.  This dataset consists of many thousands of examples of images of the digits 0 through 9.  Each such image is represented by a 28$\times$28 matrix, which means that the observed (or ambient) dimensionality of the data is $p=784$.  Because we are motivated by the $n \ll p$ scenario, we subsample the data to select $n=300$ examples of the numbers $3$, $7$, and $8$. We then apply all three approaches to this subsample of the MNIST dataset, learning a projection, and embedding $m=500$ testing samples, and classifying the resulting embedded data.

% Figure \ref{f:mnist}(A) shows several examples of digit, the border of each digit is colored for visualization purposes only.  
% Figure \ref{f:mnist}(B) shows the estimated projection matrices for each approach (see Methods for details). 
% These are the matrices that transform the data into a low-dimensional representation. 



% Figure \ref{f:mnist}(C) shows n'=500 randomly selected training samples from the digits 3, 7, and 8, embedded into the first two dimensions using each of the three approaches. The colors of the embedded points correspond to those in the leftcolumn, that is, 3 is blue, 7 is red, and 8 is green.
% FIgure \ref{f:mnist}(D) shows a smoothed histogram of the posterior probabilities of each test samples after classifying.
\sct{Lasso}, by virtue of being a sparse method, finds the pixels that most discriminate the 3 classes.  The resulting embeddings mostly live along the boundaries, because these images are close to binary, and therefore, images either have or do not have a particular pixel. Indeed, although the images themselves are nearly sparse (over 80\% of the pixels in the dataset have intensity $\leq 0.05$),  a low-dimensional discriminant boundary does not seem to be sparse.  \Pca, on the other hand, finds the linear combinations of training samples that maximize the variance.  This unsupervised linear manifold learning method results in projection matrices that indeed look like linear combinations of the three different digits.  The goal here, however, is separating classes, not maximizing variability.  The resulting embeddings are not particularly well separated, suggesting the the directions of discriminability are not the same as the directions of maximum variance.  \Lol~is our newly proposed supervised linear manifold learning method (see below for details).  The projection matrices it learns look qualitatively much like those of \Pca. This is not surprising, as both are linear combinations of the training examples.  The resulting embeddings however, look quite different.  The three different classes are very clearly separated by even the first two dimensions.  The result of these embeddings yields classifiers whose performance is obvious from looking at the embeddings: \Lol~achieves significantly smaller error than the other two approaches.  
This numerical experiment justifies the use of supervised linear manifold learning, we next investigate the performance of these methods in simpler simulated examples, to better illustrate when we can expect \Lol~to outperform other methods, and perhaps more importantly, when we expect this ``vanilla'' variant of \Lol~to fail.

% \clearpage
\subsection*{Linear Gaussian Intuition}

The above real data example suggests the geometric intuition for when \Lol~outperforms its sparse and unsupervised counterparts.  To further investigate, both theoretically and numerically, we consider the simplest setting that illustrates the relevant geometry.  In particular, we consider a two-class classification problem, where both classes are distributed according to a multivariate normal distribution, the class priors are equal, and the joint distribution is centered, so that the only difference between the classes is their means (we call this the Linear Discriminant Analysis (\Lda) model; see Methods for details).  




To motivate \Lol, and the following simulations, lets consider what the optimal projection would be in this scenario. The optimal low-dimensional projection is analytically available as the dot product of the difference of means and the inverse covariance matrix, $\mb{A}_*=\mb{\delta}\T \bSig^{-1}$ \cite{Bickel2004a} (see Methods for details).  
\Pca, the dominant unsupervised manifold learning method, utilizes only the covariance structure of the data, and ignores the difference between the means.  
In particular, \Pca~would project the data on the top d eigenvectors of the  covariance matrix.
\textbf{The key insight of our work is the following: we can combine the difference of the means and the covariance matrix in a simple fashion, rather than just the covariance matrix, to find a low dimensional projection.}  
Nai\"vely, this should typically improve performance, because in this stylized scenario, both are important. The \sct{F2M} literature has a similar insight, but a different construction that requires the dimensionality to be smaller than the sample size.  Formally, we implement our idea by simply concatenating the difference of the means with the top d eigenvectors of the  covariance. 
This is equivalent to first projecting onto the difference of the means vector, and then projecting the residuals onto the first d principle components. 
Thus, it requires almost no additional computational time or complexity over that of \Pca, rather, merely estimates the difference of the means.  
In this sense, \Lol~can be thought of as a very simple  ``supervised \Pca''.  



\begin{figure}%{R}{0.7\textwidth}%[h!]
% \begin{SCfigure}
\centering
\includegraphics[width=0.8\linewidth,trim=0in 0in 1.5in 0in,clip=true]{../Figs/cigars_est}%l b r t
\caption{
\Lol~achieves near optimal performance for a wide variety of distributions. 
Each point is sampled from a multivariate Gaussian; 
the three columns correspond to different simulation parameters (see Methods for details).  
In each of 3 simulations, we sample $n=100$ points in $p=1000$ dimensions.  And for each approach, we embed into the top 20 dimensions. Note that we use the sample estimates, rather than the true population values of the parameters.  In this setting, the results are  similar.
\textbf{(A)}: The mean difference vector is aligned with the direction of maximal variance, maxing it ideal for both \Pca~to discover the discriminant direction and a sparse solution.  
\textbf{(B)}: The mean difference vector is orthogonal to the direction of maximal variance, making \Pca~fail, but sparse methods can still recover the correct dimensions.
\textbf{(C)}: Same as (B), but the data are rotated.  
\textbf{Row 1}: A scatter plot of the first two dimensions of the sampled points, with class 0 and 1 as black and gray dots, respectively.  
\textbf{Row 2}: \sct{Fld $\circ$ Pca}.
\textbf{Row 3}: \sct{Road}, a sparse method designed specifically for this model \cite{Fan2012a}.
\textbf{Row 4}: \Lol, our newly proposed method.
\textbf{Row 5}: the Bayes optimal classifier, which is what all classifiers strive to achieve.
Note that \Lol~is closest to Bayes optimal in all three settings.
}
\label{f:cigars}
\end{figure}
% \vspace{-30pt}


Figure \ref{f:cigars} shows three different examples of data sampled from the \Lda~model to geometrically illustration this intuition.
In each, we sample $n=100$ training samples in $p=1000$ dimensional space, so $n \ll p$.  
Figure \ref{f:cigars}(A) shows an example we call  ``stacked cigars''. 
In this example and the next, the covariance matrix is diagonal, so all ambient dimensions are independent of one another.  
Moreover,  the difference between the means and direction of maximum variance are both large along the same dimensions (they are highly correlated with one another). 
This is an idealized setting for \Pca, because \Pca~finds the direction of maximal variance, which happens to correspond to the direction of maximal separation.  
However, in the face of high-dimensional data, \Pca~does not weight the discriminant directions sufficiently, and therefore performs only moderately well.\footnote{When having to estimate the eigenvector from the data, \Pca~performs even worse.  This is because when $n \ll p$, \Pca~is an inconsistent estimator with large variance \cite{Baik2006a,Paul2007a}}
Because all dimensions are independent, this is a good scenario for sparse methods.  
Indeed,  \sct{Road}, a sparse classifier designed for precisely this scenario,  does an excellent job finding the most useful ambient dimensions.  
\Lol~does the best of all three approaches, by using both the difference of the means and the covariance.


Figure \ref{f:cigars}(B) shows an example which is a worst case scenario for using \Pca~to find the optimal projection for classification.  
In particular, the variance is getting larger for subsequent dimensions, $\sigma_1 < \sigma_2 < \cdots < \sigma_p$, while the magnitudes of the difference between the means are decreasing with dimension, $\delta_1 > \delta_2 < \cdots > \delta_p$. 
Thus, for any truncation level,  \Pca~finds exactly the \emph{wrong} directions.  
\sct{Road} is not hampered by this problem, it is also able to find the directions of maximal discrimination, rather than those of maximal variance.
Again, \Lol, by using both parameters, does extremely well.


Figure \ref{f:cigars}(C) is exactly the same as (B), except the data have been randomly rotated in all 1000 dimensions.  This means that none of the original coordinates have much information, rather, linear combinations of them do.  
This is evidenced by observing the scatter plot, which shows that two dimensions clearly fail to disambiguate the two classes.
\Pca, being rotationally invariant, fails in this scenario as it did in (B).
Now, there is no small number of ambient dimensions that separate the data well, so \sct{Road} also fails.
 \Lol is unperturbed by this rotation; in particular, it is able to ``unrotate'' the data, to find dimensions that optimally separate the two classes.  


\subsection*{Theoretical Confirmation}

The above numerical experiments provide the intuition to guide our theoretical developments.  
\begin{thm} \label{t:LDA}
Under the \Lda~model, \Lol~is better than \Pca.
\end{thm}
In words, it is better to incorporate the mean difference vector into the projection matrix.  The degree of improvement is a function of the embedding dimension d, the ambient dimensionality p, and the parameters (see Methods for details and proof).



\subsection*{How many dimensions to keep?}

In the above numerical and theoretical investigations, we fixed $d$, the number of dimensions to embed into.  Much unsupervised manifold learning theory typically focuses on finding the ``true'' intrinsic dimensionality of the data.   The analogous question for supervised manifold learning would be to find the true intrinsic dimensionality of the discriminant boundary.  However, in real data problems, typically, their is no perfect low dimensional representation because of noise.
% , rather, the more data we obtain, the higher-dimensional discriminant boundary we can estimate, and the close to Bayes optimal we can perform.  

Thus, in all the following simulations, the true ambient dimensionality of the data is equal to the dimensionality of the optimal discriminant boundary (given infinite data).  In other words, there does not exist a discriminant space that is lower dimensional than the ambient space, so we cannot find the ``intrinsic dimension'' of the data or the discriminant boundary.  Rather, we face a trade-off: keeping more dimensions reduces bias, but increases variance.  The optimal bias/variance trade-off depends on the distribution of the data, as well as the sample size \cite{Trunk1979a}.  

% We formalize this notion for the \Lda model and proof the following:
% \begin{thm} \label{t:n}
% Under the \Lda~ model, estimated \Lol~is better than \Pca.
% \end{thm}
% Note that the degree of improvement is a function of the number of samples $n$, in addition to the embedding dimension $d$, the ambient dimensionality $p$, and the parameters (see Methods for details and proof).


Consider again the rotated trunk example as well as a ``Toeplitz'' example, as depicted in Figures \ref{f:properties}(A) and (B), respectively.  In both cases, the data are sampled from the \Lda~ model, and in both cases, the optimal dimensionality under finite samples depends on the particular approach, but is never the true dimensionality.  Moreover, \Lol~dominates the other approaches, regardless of the number of dimensions used.
% 
Figure \ref{f:properties}(C) shows a sparse example with ``fat tails'' to mirror real data settings better.  The qualitative results are consistent with those of (A) and (B), even though the setting is no longer exactly the setting under which we have theoretical confirmation.

% Indeed, we can generalize Theorem \ref{t:n} to include ``sub-Gaussian'' data, rather than just Gaussian:
% \begin{thm} \label{t:FAT}
% Under a sub-Gaussian generalization of the \Lda~ model, \Lol~is still better than \Pca.
% \end{thm}

\subsection*{Multiple Classes}

\Lol~can trivially be extended to $>2$ class situations.  Nai\"vely it may seem like we would need to keep all pairwise differences between means.  However, given $k$ classes, the set of all $k^2$ differences is only rank $k-1$.  In other words, we can equivalently find the class which has the maximum number of samples (breaking ties randomly), and subtract its mean from all other class means.  Figure \ref{f:properties}(D) shows a 3-class generalization of (A).  While \Lol~uses the additional class naturally,  many previously proposed high-dimensional \Fld~variants, such as \sct{Road}, natively only work for 2-classes. 


\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{../Figs/properties}
\caption{
Seven simulations demonstrating that even when the true discriminant boundary is high-dimensional, \Lol~can find a low-dimensional projection that wins the bias-variance trade-off against competing methods.  
For the first four, the top panels depict the means (top), the shared covariance matrix (middle).  For the next three, the top panels depict a 2D scatter plot (left), mean and level set of one standard deviation of covariance matrix (right).  For all seven simulations, the bottom panel shows misclassification rate as a function of the number of embedded dimensions, for several different classifiers.  The simulations settings are as follows:
\textbf{(A)} Rotated Trunk: same as Figure \ref{f:cigars}(C).
\textbf{(B)} Toeplitz: another setting where mean difference is not well correlated with any eigenvector, and no ambient coordinate is particularly useful on its own.
\textbf{(C)} Fat Tails: a common phenomenon in real data; we have theory to support this generalization of the \Lda~ model.
\textbf{(D)} 3 Classes: \Lol~naturally adapts to multiple classes.
\textbf{(E)} QDA: QOQ, a variant of \Lol~when each class has a unique covariance, outperforms \Lol, as expected.
\textbf{(F)} Outliers: adding high-dimensional outliers degrades performance of standard eigensolvers, but those can easily be replaced in \Lol~for a robust variants (called \Lrl).
\textbf{(F)} XOR: a high-dimensional stochastic generalization of XOR, demonstrating the \Lol~and QOQ work even in scenarios that are quite distinct from the original motivating problems.
% While (A) and (B) are both Gaussian models with a shared covariance matrix, (C) is a sparse example with fat tails, that is, tails decreasing less quickly than a Gaussian, which is a more general setting. And (D) shows a 3 class generalization of (A).
% In all four settings, the vanilla \Lol~method 
% (\sct{p,E,N}) 
% outperforms the \Pca~and sparse methods for nearly all settings. Note that \Pca~can be thought of as a special case of \Lol, specifically \sct{N,E,N}.
% Simulations (E) through (G) demonstrate additional generalizations, including (E) each class having a unique covariance matrix (the QDA model), (F) data including outliers, and (G) highly nonlinear discrimination boundary (a multivariate generalization of XOR).
% and error rate as a function of embedded dimension (bottom panel). 
% A version of \Lol~designed for each setting is shown to improve over the vanilla \Lol, as expected. 
In all 7 cases, \Lol, or the appropriate generalization thereof, outperforms unsupervised, sparse, or other methods.  Moreover, the optimal embedding dimension is never the true discriminant dimension, but rather, a smaller number jointly determined by parameter settings and sample size.
}
\label{f:properties}
\end{figure}



\subsection*{Generalizations of \Lol}

The simple geometric intuition which led to the development of \Lol~suggests that we can easily generalize \Lol~to be more appropriate for more complicated settings. We consider three additional scenarios:

\para{QDA} Sometimes, it makes more sense to model each class as having a unique covariance matrix, rather than a shared covariance matrix.  Assuming everything is Gaussian, the optimal classifier in this scenario is called  Quadratic Discriminant Analysis (QDA) \cite{Hastie2004}.  Intuitively then, we can modify \Lol~to compute the eigenvectors separately for each class, and concatenate them (sorting them according to their singular values).  Moreover, rather than classifying the projected data with \Lda, we can then classify the projected data with QDA.  Indeed, simulating data according to such a model (Figure \ref{f:properties}(E)), \Lol~performs slightly better than chance, regardless of the number of dimensions we use to project, whereas QOQ (which denotes we estimate eigenvectors separately and then use QDA on the projected data) performs significantly better regardless of how many dimensions it keeps.

\para{Outliers}  Outliers persist in many real data sets.  Finding outliers, especially in high-dimensional data, is both tedious and difficult.  Therefore, it is often advantageous to have estimators that are robust to certain kinds of outliers \cite{Huber1981a,Rousseeuw1999a,Ferrari2010a}.  \Pca~and eigenvector computation are particularly sensitive to outliers \cite{Candes2009b}.  Because \Lol~is so simple and modular, we can replace typical eigenvector computation with a robust variant thereof, such as the geometric median subspace embedding \cite{Zhang2014a}.  Figure \ref{f:properties}(F) shows an example where we generated  $n/2$ training samples according to the simple \Lda~ model, but then added another $n/2$ training samples from a noise model.  \Lrl~(our robust variant of \Lol~that simply replaces the fragile eigenvector computation with a robust version), performs better than \Lol~regardless of the number of dimensions we keep.

\para{XOR}  XOR is perhaps the simplest nonlinear problem, the problem that led to the demise of the perceptron, prior to its resurgence after the development of multi-layer perceptrons \cite{Bishop2006a}.  Thus, in our opinion, it is warranted to check whether any new classification method can perform well in this scenario.  The classical (two-dimensional) XOR problem is quite simple: the output of a classifier is zero if both inputs are the same (00 or 11), and the output is one if the inputs differ (01 or 10).  Figure \ref{f:properties}(G) shows a high dimensional and stochastic variant of XOR.  This simulation was designed such that standard classifiers, such as support vector machines and random forests, achieve chance levels (not shown).  \Lol, performs moderately better than chance, and QOQ performs significantly better than chance, regardless of the chosen dimensionality.  This demonstrates that our classifiers developed herein, though quite simple and intuition, can perform well even in settings where the data are badly modeled by our underlying assumptions.  This mirrors previous findings where the so-called ``idiots's Bayes'' classifier outperforms more sophisticated classifiers \cite{Bickel2004a}.  In fact, we think of our work as finding intermediate points between idiot's Bayes (or nai\"ve Bayes) and \Fld, by enabling degrees of regularization by changing the dimensionality used.


\subsection*{Computational Efficiency}

In many applications, the main quantifiable consideration in whether to use a particular method, other than accuracy, is computational efficiency.  Because implementing \Lol~requires only highly optimized linear algebraic routines---including computing moments and singular value decomposition---rather than the costly iterative programming techniques currently required for sparse or dictionary learning type problems.  To quantify the computational efficiency of \Lol~and its variants, Figure \ref{f:speed} shows the wall time it takes to run each method on the stacked cigars problem, varying the ambient dimensionality, embedded dimensionality, and sample size.  Note that for completeness, we include two additional variants of \Lol: \Lal~and \Lfl.  \Lfl~(short for Linear Fast Low-rank) replaces the standard \Svd~algorithm with a randomized variant, which can be much faster in certain situations \cite{Halko2011a}.  \Lal~(short for Linear Approximate Low-rank) goes even one step further, replacing \Svd~with random projections \cite{Candes2006a}.  This variant of \Lol~is the fastest, its runtime is  least sensitive to $(p,d,n)$, and its accuracy is often commensurate (or better) than other variants of \Lol.  The runtime of all the variants of \Lol~are quite similar to \sct{Fld $\circ$ Pca}.  Given, given \Lol's improved accuracy, and nearly identical simplicity, it seems there is very little reason to not use \Lol~instead of \sct{Fld $\circ$ Pca}.

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{../Figs/speed_test}
\caption{
Computational efficiency of various low-dimensional projection methods. In all cases, $n=100$, and we used the ``stacked cigars'' simulation parameters.   We compare \Pca~with the projection steps from \Lol, \Qoq, \Lrl, \Lfl, and \Lal, for different values of $(p,d)$.  The addition of the mean difference vector is essentially negligible.  Moreover, for small $d$, the \Lfl~is advantageous.  \Lal~is always fastest, and its performance is often comparable to other methods (not shown).
}
\label{f:speed}
\end{figure}




\subsection*{Benchmark Real Data Applications}


To more comprehensively understand the relative advantages and disadvantages of \Lol~with respect to other high-dimensional classification approaches, in addition to evaluating its performance in theory, and in a variety of numerical simulations, 
 it is important to evaluate it also on benchmark datasets.  For these purposes, we have selected four commonly used high-dimensional datasets (see Methods for details).  For each, we compare \Lol~to (i) support vector machines (SVM), (ii) \sct{Road}, (iii) lasso, (iv) and random forest (RF).  Because in practice all these approaches have ``hyperparameters'' to tune, we consider several possible values for  SVM, lasso, and \Lol~(but not RF, as its runtime was too high).  Figure \ref{f:realdata} shows the results for all four datasets.  



\begin{figure}
\centering
\includegraphics[width=1\linewidth]{../Figs/realdata}
\caption{
For four standard datasets, we benchmark \Lol~(green circles) versus standard classification methods, including support vector machines (blue up triangles), \sct{Road} (cyan down triangles), \sct{Lasso}~(magenta pluses), and random forest (orange diamonds). 
Top panels show error rate as a function of log$_2$ number of embedded dimensions (for \Lol, \sct{Road}, and \sct{Lasso}) or cost (for SVM).
% \Lol~consistently achieves lower error for fewer dimensions.
Bottom panels show the minimum error rate achieved by each of the five algorithms versus time.
The lower left dark gray (upper right light gray) rectangle is the area in which any algorithm is \emph{better}  (worse) than \Lol~in terms of both accuracy and efficiency.
\textbf{(A)} Prostate: a standard sparse dataset.  1-dimensional \Lol~does very well, although keeping $2^5$ ambient coordinates slightly improves performance, at a significant cost of compute time (two orders of magnitude), with minimal additional interpretability.
\textbf{(B)} Colon: another standard sparse dataset.  Here, 2-4 dimensions of \Lol~outperforms all other approaches considered.
\textbf{(C)} MNIST: 10 image categories here, so \sct{Road} is not possible.  \Lol~does very well regardless of the number of dimensions kept.  SVN marginally improves on \Lol~accuracy, at a significant cost in computation (two orders of magnitude).
\textbf{(D)} CIFAR-10: a higher dimensional and newer 10 category image classification problem.  Results are qualitatively similar to (C).
% 
Note that, for none of the problems is there an algorithm ever performing better and faster than \Lol; rather, most algorithms typically perform worse and slower (though some are more accurate and much more computationally expensive. 
This suggests that regardless of how one subjectively weights computational efficiency versus accuracy, \Lol~is the best default algorithm in a variety of real data settings.
% In many real data applications, the difference of the means contains much of the discriminant manifold.  We subsampled 100 points from the MNIST dataset, consisting of the numbers 3, 7, and 8, and applied both \Lol~and \Pca~to embed the data into a small dimension.  Clearly, the first two dimensions of \Lol~(left panel) separate these data far better than the first few dimensions of \Pca~(middle panel).  This translates into vastly superior performance in subsequent classification (right panel).
}
\label{f:realdata}
\end{figure}



Qualitatively, the results are similar across datasets: \Lol~achieves high accuracy and computational efficiency as compared to the other methodologies.  Considering Figure \ref{f:realdata}(A) and (B), two popular sparse settings, we find that \Lol~can find very low dimensional projections with very good accuracy. For the prostate data, with a sufficiently non-sparse solution for \sct{Road}, it slightly outperforms \Lol, but at substantial computational cost, in particular, \sct{Road} takes about 100 times longer to run on this dataset.   Figure \ref{f:realdata}(C) and (D) are 10-class problems, so \sct{Road} is no longer possible.  Here, SVM can again slightly outperform \Lol, but again, requiring 100 fold additional computational time.  In all cases, the beloved random forest classifier performs subpar.


\subsection*{Extensions to Other Supervised Learning Problems}

The utility of incorporating the mean difference vector into supervised machine learning for wide data extends beyond merely classification.  In particular, hypothesis testing can be considered as a special case of classification, with a particular loss function.  Therefore we apply the same idea to a hypothesis testing scenario.  The multivariate generalization of the t-test, called Hotelling's Test, suffers from the same problem as does the classification problem; namely, it requires inverting an estimate of the covariance matrix. To mitigate this issue in the hypothesis testing scenario, prior art applied similar tricks as they have done in the classification setting. One particularly nice and related example is that of  Lopes et al. \cite{Lopes2011a}, who addresses this dilemma by using random projections to obtain a low-dimensional representation, following by applying Hotelling's Test in the lower dimensional subspace.  Figure \ref{f:generalizations}(A) and (B) shows the power of their test alongside the power of the same approach, but using the \Lol~projection rather than random projections.  The two different simulations include the simulated settings considered in their manuscript (see Methods for details).  The results make it clear that the \Lol~test has higher power for essentially all scenarios.  Moreover, it is not merely the replacing random projections with \Pca (solid magenta line), nor simply incorporating the mean difference vector (dashed green line), but rather, it appears that \Lol~for testing uses both modifications to improve performance.

High-dimensional linear regression is another supervised learning method that can utilize this idea. Linear regression, like classification and Hotelling's Test, requires inverting a singular matrix as well.  By projecting the data only a lower dimensional subspace first, followed by linear regression on the low-dimensional data, we can mitigate the curse of high-dimensions.  To choose the projection matrix, we partition the data into K partitions, based on the percentile of the target variable, we obtain a K class classification problem.  Then, we can apply \Lol~to learn the embedding.  Figure \ref{f:generalizations}(C) shows an example of this approach, contrasted with lasso and partial least squares, in a sparse simulation setting (see Methods for details). \Lol~is able to find a better low-dimensional projection than lasso, and performs significantly better than PLS, for essentially all choices of number of dimensions to embed into.




\begin{figure}
% \begin{wrapfigure}{R}{0.7\textwidth} %[h!]
\centering
\includegraphics[width=1\linewidth]{../Figs/regression_power}
\caption{
The intuition of including the mean difference vector is equally useful for other supervised manifold learning problems, including testing and regression.
(A) and (B) show two different high-dimensional testing settings, as described in Methods.  Power is plotted against the decay rate of the spectrum, which approximates the effective number of dimensions.  \Lol~composed with Hotelling's test outperforms the random projections variants described in \cite{Lopes2011a}, as well as several other variants.
(C) shows a high-dimensional regression settings, as described in Methods.  Log$_{10}$ mean squared error is plotted against the number of embedded dimensions.  
Regression \Lol~composed with linear regression outperforms \sct{Lasso}~(cyan), the classic sparse regression method, as well as partial least squares (PLS; black).
In the legend, 'A' denote either 'linear regression' (in (C)), or 'Hotelling' (in (A) and (B)).
These three simulation settings therefore demonstrate the generality of this technique.
}
\label{f:generalizations}
% \end{wrapfigure}
\end{figure}
% \clearpage


\section*{Discussion}


We have introduced a very simple, yet new, methodology to improve performance on supervised learning problems with wide data.  In particular, we have proposed a supervised manifold learning procedure, the utilizes both the difference of the means, and the covariance matrices.  This is in stark contrast to previous approaches, which only utilize the covariance matrices (or kernel variants thereof), or solve a difficult optimization theoretic problem.  In addition to demonstrating the accuracy and numerical efficiency of \Lol~on simulated and real classification problems, we also demonstrate how the same idea can also be used for other kinds of supervised learning problems, including regression and hypothesis testing. Theoretical guarantees suggest that this line of research is promising and can be extended to other more general settings and tasks.

One of the first publications to compose \Fld~with an unsupervised learning method was the celebrated Fisherfaces paper \cite{Belhumeur1997a}.  The authors showed via a sequence of numerical experiments the utility of embedding with \Pca~prior to classifying with \Fld.  We extend this work by adding a supervised component to the initial embedding.  Moreover, we provide the geometric intuition for why and when this is advantageous, as well as show numerous examples demonstrating its superiority.  %Finally, we have matrix concentration inequalities proving the advantages of \Lol~over Fisherfaces.  


The \Lol~idea, appending the mean difference vector to convert unsupervised manifold learning to supervised manifold learning, has many potential applications.  We have presented the first few.  Incorporating additional nonlinearities via kernel methods \cite{Mika1999a}, ensemble methods such as random forests \cite{Breiman2001a}, multiscale methods \cite{Allard2012},  and more scalable implementations \cite{Chang2011a}, are all of immediate interest.


\clearpage

\begin{figure}
\centering
\includegraphics[width=1\linewidth,trim=0.5in 4.5in 0.5in 0.5in,clip=true]{../Figs/table} %l b r t
\caption{Table of algorithms and their properties for high-dimensional data. Gray elements indicate that results are demonstrated in the Figure labeled in the bottom row. 'X' denotes relatively good performance for a given setting, or has the particular property.
}
\label{f:table}
\end{figure}







\clearpage
\appendix
\section{Theoretical Background}


\subsection{The Classification Problem}

Let $(\bX,Y)$ be a pair of random variables, jointly sampled from $F :=F_{\bX,Y}=F_{\bX|Y}F_{Y}$.  
Let $\bX$ be a multivariate vector-valued random variable, such that its realizations live in p dimensional Euclidean space, $\bx \in \Real^p$.  Let $Y$ be a categorical random variable, whose realizations are discrete,  $y \in \{0,1,\ldots C\}$.  The goal of a classification problem is to find a function $g(\bx)$ such that its output tends to be the true class label $y$:
\begin{align*} %\label{eq:bayes}
g^*(\bx) := \argmax_{g \in \mc{G}} \PP[g(\bx) = y].
\end{align*}
When the joint distribution of the data is known, then the Bayes optimal solution is:
\begin{align}  \label{eq:R}
g^*(\bx) := \argmax_y f_{y|\bx} = \argmax_y f_{\bx|y}f_y =\argmax_y \{\log f_{\bx|y} + \log f_y \}
\end{align}
Denote expected misclassification rate of classifier $g$ for a given joint distribution $F$, 
\begin{align*}
L^F_g := \EE[g(\bx) \neq y] := \int \PP[g(\bx) \neq y] f_{\bx,y} d\bx dy,
\end{align*}
where $\EE$ is the expectation, which in this case, is with respect to $F_{XY}$.
For brevity, we often simply write $L_g$, and we define $L_* := L_{g^*}$.  


\subsection{Linear Discriminant Analysis (\Lda)}

Linear Discriminant Analysis (\Lda) is an approach to classification that uses a linear function of the first two moments of the distribution of the data.  More specifically, let $\mu_j=\EE[F_{X|Y=j}]$ denote the class conditional mean, and let $\bSig=\EE[F_{X}^2]$ denote the joint covariance matrix, and $\pi_j=\PP[Y=j]$.   Using this notation, we can define the \Lda~classifier:
\begin{align*}
g_{\Lda}(\bx)&:=\argmin_y \frac{1}{2} (\bx-\bmu_0)\T \bSig^{-1}(\bx-\bmu_0) + \II\{Y=y\}  \log \pi_y,
\end{align*}  
where $\II\{ \cdot\}$ is one when its argument is true, and zero otherwise.
Let $L_{\Lda}^F$ be the misclassification rate of the above classifier for distribution $F$.
% 
Assuming equal class prior and centered means,  $\pi_0=\pi_1$ and $(\bmu_0+\bmu1)/2=\mb{0}$, re-arranging a bit, we obtain
\begin{align*}
g_{\Lda}(\bx) :=  \argmin_y \bx\T \bSig^{-1} \bmu_y.
\end{align*}
In words, the  \Lda~classifier chooses the class for whom the projection of an input vector $\bx$, onto $\bSig^{-1} \bmu_y$, is maximized.
% 
When there are only two classes, this further simplies to 
\begin{align*}
g_{2-\Lda}(\bx) :=  \II\{ \bx\T \bSig^{-1} \bdel > 0 \},
\end{align*}
where $\bdel=\bmu_0-\bmu_1$.   Note that the equal class prior and centered means assumptions merely changes the threshold constant from $0$ to something else.  

\subsection{\Lda~Model}

A statistical model is  a family of distributions indexed by a parameter $\bth \in \bTh$, $\mc{F}_{\bth}=\{F_{\bth} : \bth \in \bTh \}$.  
Consider the special case of the above where $F_{\bX|Y=y}$ is a multivariate Gaussian distribution, 
$\mc{N}(\bmu_y,\bSig)$, where each class has its own mean, but all classes have the same covariance. 
We refer to this model as the \Lda~model. 
Let $\bth=(\bpi,\bmu,\bSig)$, and let $\bTh_{C-\Lda}=( \triangle_C, \Real^{p \times C},\Real_{\succ 0}^{p \times p})$, where $\bmu=(\bmu_1,\ldots, \bmu_C)$, $\triangle_C$ is the $C$ dimensional simplex, that is $\triangle_C = \{ \bx : x_i \geq 0 \forall i, \sum_i x_i = 1\}$, and $\Real_{\succ 0}^{p \times p}$ is the set of positive definite  $p \times p$ matrices. Denote
$\mc{F}_{\Lda}=\{F_{\bth} : \bth \in \bTh_{\Lda}\}$, dropping the superscript $C$ for brevity where appropriate.
The following lemma is well known:
\begin{lem}
$L_{\Lda}^F=L_*^F$ for any $F \in \mc{F}_{\Lda}$.
\end{lem}

\begin{proof}
Under the \Lda~model, the Bayes optimal classifier is available by plugging the explicit distributions into Eq.~\eqref{eq:R}.
\end{proof}




\section{Projection Based Classifiers}


Let $\bA \in \Real^{d \times p}$ be an orthonormal matrix, that is, a matrix that projects p dimensional data into a d dimensional subspace, where $\bA\bA\T$ is the $d \times d$ identity matrix, and $\bA\T \bA$ skis symmetric $p \times p$ matrix with rank d.   The question that motivated this work is: what is the best projection matrix that we can estimate, to use to ``pre-process'' the data prior to applying \Lda.  
% \begin{lem}
% $g_{\Lda}^F(\bA \bx)= \II \{ (\bA \bx)\T \bSig^{-1}_A \bdel_A > 0\}$.
% \end{lem}
Projecting the data $\bx$ onto a low-dimensional subspace, and the classifying via \Lda~in that subspace is equivalent to redefining the parameters in the low-dimensional subspace, 
$\bSig_A=\bA \bSig \bA\T \in \Real^{d \times d}$ and $\bdel_A = \bA \bdel \in \Real^d$, and then using $g_{\Lda}$.  When $C=2$, $\pi_0=\pi_1$, and $(\mu_0+\mu_1)/2=\mb{0}$, this amounts to:
\begin{align} \label{eq:g_A}
g^d_A(x) := \II \{ (\bA \bx)\T \bSig^{-1}_A \bdel_A > 0\}, \text{ where } \bA \in \Real^{d \times p}.
\end{align}
Let $L^d_A :=\int \PP[g_A(\bx)=y] f_{\bx,y} d\bx dy$.
Our goal therefore is to be able to choose $A$ for a given parameter setting $\bth=(\bpi, \bdel,\bSig)$, such that $L_A$ is as small as possible (note that $L_A$ will never be smaller than $L_*$).  

Formally, we seek to solve the following optimization problem:
% \begin{align} \label{eq:A}
% \bA_* = \argmin_{\bA \in \Real^{p \times d}} L_A.
% \end{align}
\begin{equation} \label{eq:A}
\begin{aligned}
& \underset{\bA}{\text{minimize}}
& & \EE [ \II \{ \bx\T \bA\T \bSig^{-1}_A \bdel_A > 0\} \neq y] \\
& \text{subject to} & & \bA \in \Real^{p \times d}, \quad \bA \bA\T = \bI_{d \times d},
\end{aligned}
\end{equation}
where $\bI_{u \times v}$ is the $u \times v$ identity matrix identity, that is, $\bI(i,j)=1$ for all $i=j \leq \min(u,v)$, and zero otherwise. 
Let $\mc{A}^d=\{\bA : \bA \in \Real^{d \times p}, \bA \bA\T = \bI_{d \times d}\}$, and let $\mc{A}_* \subset \mc{A}$ be the set of $\bA$  that minimize Eq.~\eqref{eq:A}, and let $\bA_* \in \mc{A}_*$ (where we dropped the superscript $d$ for brevity).   Let $L_{\bA}^*=L_{\bA_*}$ be the misclassification rate for any $\bA \in \mc{A}_*$, that is, $L_{\bA}^*$ is the Bayes optimal misclassification rate for the classifier that composes $\bA$ with \Lda.


In our opinion, Eq.~\eqref{eq:A} is the simplest supervised manifold learning problem there is: a two-class classification problem, where the data are multivariate Gaussians with shared covariances, the manifold is linear, and the classification is done via \Lda.
Nonetheless, solving Eq.~\eqref{eq:A} is difficult, because we do not know how to evaluate the integral analytically, and we do not know any algorithms that are guaranteed to find the global optimum in finite time.  This has led to previous work using a surrogate function \cite{not sure who}.  
We proceed by studying a few natural choices for $\bA$.





\subsection{Bayes Optimal Projection}

% Let $\mb{\bA}\T=\bSig^{-1} \bdel$.
\begin{lem}
$\bdel\T  \bSig^{-1} \in \mc{A}_*$
\end{lem}

\begin{proof}
Let $\bB = (\bSig^{-1} \bdel)\T = \bdel\T (\bSig^{-1})\T = \bdel\T \bSig^{-1}$, so that $\bB\T = \bSig^{-1} \bdel$,
% = \bdel\T \bOm\T
and plugging this in to Eq.~\eqref{eq:g_A}, we obtain
\begin{align*}
g_{B}(x) &= \II \{ \bx \bB\T  \bSig^{-1}_{B} \bdel_{B} > 0\} &
\\&= \II \{ \bx\T \bSig^{-1} \bdel \times (\bSig^{-1}_{B} \bdel_{B}) > 0\} & \text{plugging in $\bB$}
\\&= \II \{ \bx\T \bSig^{-1} \bdel k > 0\} & \text{because $\bSig^{-1}_{B} \bdel_{B} > 0$}.
\end{align*}
In other words, letting $\bB$ be the Bayes optimal projection recovers the Bayes classifier, as it should.
Or, more formally, for any $F \in \mc{F}_{\Lda}$, $L_{\bdel\T \bSig^{-1}} = L_*$ 
\end{proof}

\subsection[PCA]{Principle Components Analysis (\Pca) Projection}

Principle Components Analysis (\Pca) finds the directions of maximal variance in a dataset.  \Pca~is closely related to eigendecompositions and singular value decompositions (\Svd).  In particular, the top principle component of a matrix $\bX \in \Real^{p \times n}$, whose columns are centered, is the eigenvector with the largest corresponding eigenvalue of the centered covariance matrix $\bX \bX\T$.  \Svd~enables one to estimate this eigenvector without ever forming the outer product matrix, because \Svd~factorizes a matrix $\bX$ into $\bU \bS \bV\T$, where  $\bU$ and $\bV$ are orthonormal  ${p \times n}$ matrices, and $\bS$ is a diagonal matrix, whose diagonal values are decreasing,  $s_1 \geq s_2 \geq \cdots > s_n$.  Defining $\bU =[\bu_1, \bu_2, \ldots, \bu_n]$, where each $\bu_i \in \Real^p$, then $\bu_i$ is the $i^{th}$ eigenvector, and $s_i$ is the square root of the $i^{th}$ eigenvalue of $\bX \bX\T$.  Let $\bA^{\Pca}_d =[\bu_1, \ldots , \bu_d]$ be the truncated \Pca~orthonormal matrix.

The \Pca~matrix is perhaps the most obvious choice of a orthonormal matrix for several reasons.  First, truncated \Pca~minimizes the squared error loss between the original data matrix and all possible rank d representations:
\begin{align*}
\argmin_{A \in \Real^{d \times p} : \bA \bA\T = \bI_{d \times d}} \norm{ \bX - \bA^T \bA }_F^2.
\end{align*}
Second, the ubiquity of \Pca~has led to a large number of highly optimized numerical libraries for computing \Pca~(for example, LAPACK \cite{Anderson1999a}). 

Moreover, let $\bU_d=[\bu_1,\ldots,\bu_d] \in \Real^{p \times d}$, and note that $\bU_d\T \bU_d = \bI_{d \times p}$ and $\bU_d\T \bU_d  = \bI_{p \times d}$.  Similarly, let $\bU \bS \bU\T = \bSig$, and $\bU \bS^{-1} \bU\T = \bSig^{-1}$.  Let $\bS_d$ be the matrix whose diagonal entries are the eigenvalues, up to the $d^{th}$ one, that is $\bS_d(i,j)=s_i$ for $i=j \leq d$ and zero otherwise.  Similarly, $\bSig_d=\bU \bS_d \bU\T=\bU_d \bS_d \bU_d\T$. 

Let $g_{\Pca}^d:=g_{A_{\Pca}^d}$, and let $L_{\Pca}^d:=L_{A_{\Pca}^d}$.   
And let $g_{\Lda}^d := \II \{ x \bSig_d^{-1} \bdel > 0\}$ be the regularized \Lda~classifier, that is, the \Lda~classifier, but sets the bottom $p-d$ eigenvalues to zero.

\begin{lem}
$L_{\Pca}^d = L_{\Lda}^d$.
\end{lem}

\begin{proof}
Plugging $\bU_d$ into Eq.~\eqref{eq:g_A} for $\bA$, and considering only the left side of the operand, we have
\begin{align*}
(\bA \bx)\T \bSig^{-1}_A \bdel_A &= \bx\T \bA\T \bA \bSig^{-1} \bA\T \bA \bdel,
\\&= \bx\T  \bU_d\bU_d\T \bSig^{-1} \bU_d\bU_d\T \bdel,
\\&= \bx\T  \bU_d \bU_d\T \bU \bS^{-1} \bU \bU_d\bU_d\T \bdel,
\\&= \bx\T  \bU_d \bI_{d \times p} \bS^{-1} \bI_{p \times d} \bU_d\T \bdel,
\\&= \bx\T  \bU_d \bS^{-1}_d  \bU_d\T \bdel ,
\\&= \bx\T  \bSig^{-1}_d  \bdel.
\end{align*}
\end{proof}

The implication of this lemma is that if one desires to implement Fisherfaces, rather than first learning the eigenvectors and then learning \Lda, one can instead directly implement regularized \Lda~by setting the bottom $p-d$ eigenvalues to zero.




\subsection[LOL]{Linear Optimal Low-Rank (\Lol) Projection}


The basic idea of \Lol~is to use both $\bdel$ and the top $d$ eigenvectors.  Most na\"ively, we could simply concatenate the two, $\bA_{\Lol}^d=[\bdel,\bA_{\Pca}^{d-1}]$.  
Recall that eigenvectors are orthonormal.  To maintain orthonormality, we could easily apply Gram-Schmidt,  $\bA_{\Lol}^d=$ \sct{Orth}$([\bdel, \bA_{\Pca}^{d-1}])$.
Both in practice and in theory (as will be shown below), this orthogonalization step does not matter much.

to ensure that they are balanced appropriately, we normalize $\bdel$

each vector in $\bdel$ to have norm unity.  Formally, let $\mt{\bdel}_j = \bdel_j / \norm{\bdel_j}$, where $\bdel_j$ is the $j^{th}$ difference of the mean vector (remember, the number of vectors is equal to $C-1$, where $C$ is the total number of classes), and let  $\bA_{\Lol}^d=[\mt{\bdel}, \bA_{\Pca}^{d-(C-1)}]$.  
The eigenvectors are all normalized and orthogonal to one another; to impose orthogonality between $\mt{\bdel}$ and the eigenvectors, we could use any number of numerically optimized algorithms.  However, in practice, orthogonalizing does not matter very much, so we do not bother. We formally demonstrate this below.




\section{Theoretical Properties of LDA based Classifiers}


\subsection{\Lda~is rotationally invariant}

For certain classification tasks, the ambient coordinates have intrinsic value, for example, when simple interpretability is desired.  However, in many other contexts, interpretability is less important \cite{Breiman2001b}.  When the exploitation task at hand is invariant to rotations, then we have no reason to restrict our search space to be sparse in the ambient coordinates, rather, for example, we can consider sparsity in the eigenvector basis.  Fisherfaces is one example of a rotationally invariant classifier, under certain model assumptions. 
Let  $\bW$ be a rotation matrix, that is $\bW \in \mc{W}=\{\bW : \bW\T = \bW^{-1}$ and det$(\bW)=1\}$. 
Moreover, let $\bW \circ F$ denote the distribution $F$ after transformation by an operator $\bW$.  For example, if $F=\mc{N}(\bmu,\bSig)$ then $\bW \circ F=\mc{N}(\bW  \bmu, \bW \bSig \bW\T)$.

\begin{defi}
A rotationally invariant classifier has the following property:
$$L_g^F = L_g^{W \circ F}, \qquad F \in \mc{F}.$$
In words, the Bayes risk of using classifier $g$ on distribution $F$ is unchanged if $F$ is first rotated, for any $F \in \mc{F}$.
\end{defi}


Now, we can state the main lemma of this subsection:  \Lda~is rotationally invariant.
\begin{lem} \label{l:rot}
$L_{\Lda}^F = L_{\Lda}^{W \circ F}$, for any $F \in \mc{F}$.
\end{lem}

\begin{proof}
\Lda~simply becomes thresholding $\bx\T \bSig^{-1} \bdel$.  Thus, we can demonstrate rotational invariance by demonstrating that $\bx\T \bSig^{-1} \bdel$ is rotationally invariant. 

% First, note that for any distribution $F \in \mc{F}_{\Lda}$, we can reparameterize it such that $\bSig$ is diagonal.  This follows because for any $\bSig$, we can represent it as $\bSig=\bU \bS \bU$, and there exists a $\bW$ such that 
% from the following:
% \begin{align}
% \bSig = \bU \bS \bU = 
% \end{align}

\begin{align*}
% \bx\T \bSig^{-1} \bdel &= 
(\bW \bx) \T  (\bW \bSig \bW\T )^{-1} \bW \bdel  %& \text{from Lemma \ref{l:rot}}\\
&= \bx\T \bW\T  (\bW \bU \bS \bU\T \bW\T)^{-1} \bW \bdel & \text{by substituting $\bU \bS \bU\T$ for $\bSig$} \\
&= \bx\T \bW\T  (\mt{\bU} \bS \mt{\bU}\T)^{-1} \bW \bdel & \text{by letting $\mt{\bU}=\bW \bU$} \\
&= \bx\T \bW\T  (\mt{\bU} \bS^{-1} \mt{\bU}\T) \bW \bdel & \text{by the laws of matrix inverse} \\
&= \bx\T \bW\T  \bW \bU \bS^{-1}  \bU\T \bW\T \bW \bdel & \text{by un-substituting $\bW \bU=\mt{\bU}$} \\
&= \bx\T  \bU \bS^{-1}  \bU\T  \bdel  & \text{because $\bW\T \bW = \bI$} \\
&= \bx\T   \bSig^{-1} \bdel & \text{by un-substituting $\bU \bS^{-1} \bU\T = \bSig$}
\end{align*}
\end{proof}

One implication of this lemma is that we can reparameterize without loss of generality.  Specifically, defining $\bW := \bU\T$ yields a change of variables: $\bSig \mapsto \bS$ and $\bdel \mapsto \bU\T \bdel := \bdel''$, where $\bS$ is a diagonal covariance matrix.  Moreover, let $\bd=(\sigma_1,\ldots, \sigma_D)\T$ be the vector of eignevalues, then $\bS^{-1} {\bdel'}=\bd^{-1} \odot \mt{\bdel}$, where $\odot$ is the Hadamard (entrywise) product.  The \Lda~classifier may therefore be encoded by a unit vector, $\mt{\bd}:= \frac{1}{m} \bd^{-1} \odot \mt{\bdel'}$, and its magnitude, $m:=\norm{\bd^{-1} \odot \mt{\bdel}}$. 
This will be useful later.




\subsection[]{Rotation of Projection Based Linear Classifiers $g_A$}

By a similar arguement as above, one can easily show that:

\begin{align*}	
(\bA  \bW \bx) \T  (\bA \bW  \bSig  \bW\T \bA\T)^{-1} \bA \bW \bdel 
&= \bx\T (\bW\T \bA\T) (\bA \bW) \bSig^{-1} (\bW\T \bA\T) (\bA \bW) \bdel \\
&= \bx\T \bY\T \bY \bSig^{-1} \bY\T \bY \bdel \\
&= \bx\T \bZ \bSig^{-1} \bZ\T \bdel \\
&= \bx\T (\bZ \bSig \bZ\T)^{-1} \bdel = \bx\T \mt{\bSig}_d^{-1} \bdel,
% (\bA\T \bA \bx) \T  \bSig^{-1} \bA\T \bA \bdel = (\bA \bx)\T \bSig^{-1}_A \bdel_A.
\end{align*}
% \end{proof}
where $\bY = \bA \bW \in \Real^{d \times p}$ so that $\bZ=\bY\T \bY$ is a symmetric ${p \times p}$ matrix of rank $d$.  In other words, rotating and then projecting is equivalent to a change of basis.
The implications of the above is: 
\begin{lem}
$g_A$ is rotationally invariant if and only if span($\bA$)=span($\bSig_d$).
In other words, \Pca~is the only rotationally invariant projection.
\end{lem}





\subsection{Simplifying the Objective Function}

Recalling Eq.~\eqref{eq:g_A}, a projection based classifier is effectively thresholding the dot product of $\bx$ with the linear projection operator $\bP_A :=\bA\T \bSig_A^{-1} \bdel_A \in \Real^p$, and let $\bP_*:=\bP_{\bA_*}$.  Unfortunately, the nonlinearity in in Eq.~\eqref{eq:A} makes analysis difficult.
However, because of the linear nature of the classifier and projection matrix operator, an objective function that is simpler to evaluate is available.
Define 
$\angle(\bP,\bP') = \frac{ \bP\T \bP'}{||\bP||_2 ||\bP'||_2} \in (0,1)$, and consider

\begin{equation} \label{eq:angle}
\begin{aligned}
& \underset{\bA}{\text{minimize}}
& & -\angle(\bP_A,\bP_*), 
\\ & \text{subject to} & & \bA \in \Real^{p \times d}, \quad \bA \bA\T = \bI_{d \times d}.
\end{aligned}
\end{equation}

\begin{lem} \label{l:angle}
The solution to Eq.~\eqref{eq:angle} is also the solution to Eq.~\eqref{eq:A} for any given $d$.
\end{lem}

\begin{proof}
The minimum of Eq.~\eqref{eq:angle} is clearly $\bA=\bSig^{-1} \bdel$, which is also the minimum of Eq.~\eqref{eq:A}.
\end{proof}


\begin{remark} 
$\angle$ is merely the angle between two vectors, and is therefore scale invariant.  In other words, $\angle(\bP_A,\bP)=\angle(\bP_{c A},\bP)$, for any $c > 0$.
\end{remark}

Given the above, we can evaluate various choices of $\bA$ in terms of their induced projection operator $\bP_A$ and the angle between said projection operators and the Bayes optimal projection operator.  
% Let $\bP_*=\bP_{A_*}=\bSig^{-1} \bdel$, 
% and $\alpha^*_A=\angle(\bP_*,\bP_A)$.


\begin{lem}
$\angle(\bP_A, \bP_*) <1  \implies L_A > L_*$
\end{lem}


\begin{proof}
If $\angle(\bP_A, \bP_*) <1$, then there exists an $\bx$ such that $\II\{ \bx\T \bP_A >0 \} \neq \II\{ \bx\T \bP_* >0 \}$, and therefore, $L_A > L_*$.
\end{proof}



\begin{conj}
% \begin{lem} 
\label{q:a2}
$$\angle(\bP_A,\bP_*) \leq \angle(\bP_B,\bP_*) \implies L_A \leq L_B.$$
% \end{lem}
\end{conj}



% Note that Conjecture \ref{q:a2} is a stronger statement than Conjecture \ref{q:a1}, and in particular, if Conjecture \ref{q:a2} is true, then so is Conjecture \ref{q:a1}.




\subsubsection{When $d=1$}

\begin{remark}
If $\bA \in \mc{A}_*$ and $\bB \notin \mc{A}_*$, then $\angle(\bA\T,\bA_*\T)=1$ and $\angle(\bB\T,\bP_*\T)<1$, and therefore $\angle(\bA\T,\bA_*\T) > \angle(\bB\T,\bA_*\T)$.
\end{remark}



\begin{conj} \label{q:angle}
When $d=1$:
% \begin{lem} 
\label{q:a1}
$$\angle(\bA\T,\bA_*\T) \leq \angle(\bB\T,\bA_*\T) \implies \angle(\bP_A,\bP_*) \leq \angle(\bP_B,\bP_*).$$
% \end{lem}
\end{conj}

\begin{proof}
I believe so, but i don't see how to prove it. A little arithmetic shows that the left hand side means that:
$$\bA \bSig^{-1} \bdel > \bB \bSig^{-1} \bdel.$$

Similarly, the right hand side means that:
$$\bdel \bA\T \bA \bSig^{-1} \bA\T a > \bdel \bB\T \bB \bSig^{-1} \bB\T b,$$
where we substituted $a:=\bA\bSig^{-1} \bdel$ and $b:=\bB \bSig^{-1} \bdel$.

But I do not see how to go any further.
\end{proof}



\newpage
\subsection{\Pca~versus \Lol} \label{sec:pvl}


We would like to prove that \Lol~is always better than \Pca, when using one or the other to project the data onto a low dimensional space, followed by classifying with \Lda, under a wide variety of of settings.  Formally, we would like to prove $\PP[L_{\Lol}^d \leq L_{\Pca}^d]$ is big.  To do so, we ask a sequence of increasingly sophisticated questions that have the following form:
\begin{compactenum}
\item Under which parameter settings is \Lol~better than \Pca?
\item How often do those parameter settings arise, under various statistical models of the parameters?
\end{compactenum}


Recall that for the $C$-class classification problem, the parameter from which we sample the data is $\bth_c=(\pi_c,\bmu_c,\bSig_c)$, where 

\begin{compactitem}
\item the class probabilities are non-negative and sum to unity: $\bpi=(\pi_1,\ldots,\pi_C) \in \triangle_C := \sum_{c \in \mc{C}} \pi_c = 1$ and $\pi_c \geq 0 \forall c \in \mc{C}$,    
\item the class means are $p$-dimensional vectors: $\bmu_c \in \Real^{p}$ is the class $c$ mean vector, and 
\item the class covariances are positive definite $p \times p$ real matrices: $\bSig_c  \in \Real^{p \times p}_+$ is the class conditional covariance matrix (and $\Real^{p \times p}_+$ is the set of positive definite real $p\times p$ matrices).  
\end{compactitem}

The fully unconstrained parameter space is therefore $\bTh=\{ \triangle_C \times (\Real^p, \Real^{p \times p}_+)^C \}$.

When using a projection based classifier, we also have the hyperparameter $d$ which specifies the the dimensionality of the low-dimensional projection.  

We thus try to answer the above two questions for increasingly relaxed constraints on the parameter space and $d$. To start, we assume that:
\begin{compactenum}
\item we have only two classes, $C=2$;
\item each class has the same covariance matrix, $\bSig_0=\bSig_1$.
\end{compactenum}

For simplicity (but without loss of generality), we will also assume that the two classes have centered means and equal priors, that is, $(\bmu_0+\bmu_1)/2=0$ and $\pi_0=\pi_1=1/2$ (relaxing this assumption merely changes the threshold of the classifier).  Let the parameter space defined by these constraints be denoted $\bTh' = \{\Real^{2p}, \Real_+^{p \times p} \}$. In Section \ref{sec:pvl}, we will always assume $\bth \in \bTh'$, unless otherwise specified.

Given the above, we consider the following sequence of relaxations, first when $d=1$, and then when $d < p$:
\begin{compactenum}
\item the covariance is a scaled identity matrix, that is, $\bSig=k \mb{I}$;
\item the covariance is a diagonal matrix, that is, $\bSig=\bS$, where $\bS_{ij}=\sigma_i$ when $i=j$ and is zero otherwise; 
\item the covariance is an arbitrary positive definite matrix, that is  $\bSig \in \Real^{p \times p}_\succ$.
\end{compactenum}

After those, extensions to the multiclass and/or different covariance matrix setting might then also be explored.








\subsubsection{$d=1$}

In this section, we will only consider $d=1$, meaning that
\begin{compactitem}
\item \Lol~is simply $\bdel$,
\item \Pca~is simply $\bu_1$, the eigenvector corresponding to the largest eigenvalue of $\bSig$.
\end{compactitem}



\paragraph{Scaled Identity Covariance Matrix}


\begin{lem}
\Lol~better than \Pca~almost always when $\bSig=k \mb{I}$.
\end{lem}

\begin{proof}
To prove this statement, we first show how to define $\bA_{\Lol}^1$ and $\bA_{\Pca}^1$ is this setting.

\begin{compactitem}
\item $\bA_{\Lol}^1$ is simply $\bdel$, regardless of the covariance matrices and priors.
\item When $\bSig=k \bI$, $\bA_{\Pca}^1$ is a random vector, because the first principal component of a scaled identity matrix can equally be defined as any basis vector.
\end{compactitem}

Thus, only when the first eigenvector of the covariance matrix is randomly assigned to $\bdel$ will \Pca~work as well as \Lol.
\end{proof}


Given the conditions under which \Lol~is better than \Pca, we next ask how often that happens, under a reasonable model assumption.

\begin{lem}
$\PP[L_{\Lol}^1 \leq L_{\Pca}^1]=1$ when $\bSig=k \bI$ and $\bdel \sim \mc{N}(\bmu_{\delta},\bSig_{\delta})$.
\end{lem}

\begin{proof}
When $\bdel \sim \mc{N}(\bmu_\delta,\bSig_\delta)$, the probability that it exactly equals any vector $\bx \in \Real^d$ is real, this includes, of course,  $\bu_1$, the first eigenvector of $k \mb{I}$.  Thus, $\PP[\bdel \propto \bu_1]=0$, no matter how $\bu_1$ is chosen (as long as it is not chosen using the knowledge of the value of $\bdel$). 
This implies that $\PP[\angle(\bu_1\T,\bA_*\T) =1 ] = 0$, and therefore, $\PP[L_{\bu_1} \geq L_*] =1$.
\end{proof}


% \newpage
\paragraph{Diagonal covariance matrix}



Now consider a simple generalization of the above scenario, namely, $\bSig=\bS$ is diagonal.
We want to know under what conditions is \Lol~better than \Pca, and how often that happens under a reasonable model. 


Let $\bs$ be the singular values of $\bSig$.  When $\bSig$ is diagonal, its diagonal elements are $s_1,\ldots,s_p$, and we call that matrix $\bS$. 
Recalling that $s_i=\lambda_i^2$, where $\lambda_i$'s are eigenvalues, and when a matrix is diagonal, $s_i=\lambda_i^2$, we have  $\bA_*=\bS^{-1} \bdel=\bs^{-1} \odot \bdel := \mt{\bdel} =(\del_1/\lambda_1^2,\ldots,\del_p/\lambda_p^2)$.



\begin{lem}
\Lol~is better than \Pca~whenever $1-\delta_1 < {s_1}/{\delta_1} \sum_{i=2}^p {\delta_i^2}/{s_i}$.
A consequence of this is that \Lol~is always better than \Pca~whenever $\delta_1>1$.
This means that when the variables are independent
\end{lem}

% \begin{question}
% When is $\angle(\bdel,\mt{\bdel}) > \angle(\bu_1, \mt{\bdel})$?
% \end{question}


\begin{proof}
Recalling that $\bu_i=\be_i$, where $\be_i$ is a vector of all zeros except a one in the $i^{th}$ element, and let $\bu^i=\bu_i$ for simplicity here, we can show that:
$$
\angle(\bdel,\mt{\bdel}) =
\sum_i \delta_i \mt{\delta}_i =
\sum_i \delta_i s^{-1}_i \delta_i =
\sum_i \delta_i^2 / s_i 
$$
and
$$
\angle(\bu^1, \mt{\bdel})  = \sum_i u^1_i \mt{\delta_i} =  \sum_i u^1_i s^{-1}_i {\delta_i}= \sum_i u^1_i {\delta_i} /s_i =  u^1_1 {\delta_1} /s_1=  {\delta_1} /s_1.
$$

Thus, $\angle(\bdel,\mt{\bdel}) > \angle(\bu_1, \mt{\bdel})$ any time that
$\sum_i \delta_i^2 / s_i > \delta_1 / s_1$. 
Note that at a minimum, it must be that $\delta_1 < 1$.

Conjectures 1 and 2 demonstrate that if the angle is better, than Bayes error is also better, and this completes the proof.
\end{proof}

% By definition, $\bA_{\Lol}^1=\bdel$, and $\bA_{\Pca}^1=\bu_1$, where $\bu_1$ is the eigenvector associated with the largest eigenvalue of $\bS$.  

% We consider a simple version of our question: how often is the angle between $\bA_{\Lol}^1$ and $\bA_*$ bigger than the angle between $\bA_{\Pca}^1$ and $\bA_*$.  Formally, we would like to prove:



If Conjecture 1 and 2 are true, then the above also implies that \Lol's Bayes error is also better than \Pca's, in this setting.
Numerical experiments suggest this is true, and moreover, that when randomly sampling $\bdel$ and $\bS$, \Lol~always does better than \Pca.

% It would be nice to be able to show, immediately, that because $\bdel$ is closer to $\mt{\delta}$ than $\bu_1$ is, that their corresponding $\bP$ matrices and classification performance are also closer to the Bayes optimal.  Numerical results suggest this is true, but I do not know how to prove it yet.

% \begin{conj}
% $\PP[ \angle(A_{\Lol}^1,\bA_*)  \geq  \angle(\bA_{\Pca}^1,\bA_*)]=1$ for any $\bth \in \bTh_{2-\Lda}$.
% \end{conj}

% \begin{proof}
% coming soon.  numerical experiments are convincing.
% \end{proof}


\begin{conj}
\Lol~is almost always better than \Pca~whenever $s_i \iid \mc{U}(0,1)$ and $\delta_i \iid \mc{N}(0,1)$ for all $i \in [p]$.
% When $\bdel \sim \mc{N}(\bmu_\delta, \bSig_\delta)$ and $\bs \sim \log \mc{N}(\bmu_s,\bSig_s)$, $\PP[\sum_i \delta_i^2 / s_i > \delta_1 / s_1]=1$.
\end{conj}
\begin{proof}
We start with the very simple scenario of $p=1$ and condition on $s$ to build intuition.
In this setting we have:
\begin{align}
\PP[ \delta^2 / s > \delta/s ] = \PP[ \delta^2 / s - \delta/s  > 0].  
\end{align}
Let $z= x+y$, where $x= \frac{\delta^2}{s}$  and $y=\frac{\delta}{-s}$.  
So, $x$ has a generalized chi-squared distribution and $y$ has a normal distribution.
A bit of algebra reveals the explicit form of this distribution, however, it includes an integral that we do not know how to evaluate.  Thus, instead of solving exactly for the above, we provide a bound.

Bennett's inequality states given $X_1,\ldots, X_n$, assume:
\begin{compactitem}
\item each $X_i$ is independent of all others
\item (without loss of generality) $\EE[X_i]=0 \forall i$
\item $|X_i|<a$ almost surely for all $i$
\item $\sigma^2 = \frac{1}{n} \sum_i \VV(X_i)$.
\end{compactitem}
Then, for any $t \geq 0$

% $$ \PP[ \sum_i X_i > t] \leq \exp \left{ -\frac{n\sigma^2}{a^2} h \left( \frac{at}{n\sigma^2}\right) \right}, $$
where $h(u)=(1+u) \log (1+u) - u$.

Recalling that $\VV(\sum_i X_i) = \sum_i \VV(X_i)$ for independent random variables, and that
\begin{compactenum}
\item  $\VV(\delta/s) = s^{-2}$
\item $\VV(\delta^2/s) = h(s)$ 
\end{compactenum}





where $\Phi$ is the cumulative distribution function for a standard normal random variable.  

Note that re-introducing randomness into $s$


Coming soon.  ``almost always'' and ``better'' mean with probability 1 and better in the sense of the angle between $\bA_{\Lol}^1$ and the Bayes vector is larger than that of $\bA_{\Pca}^1$ and the Bayes vector.
I believe this also implies Bayes error is better, but we have not yet shown that.
\end{proof}




\paragraph{Arbitrary Covariance Matrix}


\subsubsection{$d \geq 1$}
\subsubsection{Scaled Identity Covariance Matrix}
\subsubsection{Diagonal Covariance Matrix}
\subsubsection{Arbitrary Covariance Matrix}




\newpage
\section{Asymptotic Theory}

In real data problems,  the true joint distribution is unknown. Instead, what is provided is a set of training data.  We therefore assume the existence of $n$ training samples, each of which has been sampled identically and independently from the same distribution, $(\bX_i,Y_i) \iid F_{\bX,Y}$, for $i =1,2,\ldots, n$.  We can use these training samples to then estimate $f_{x|y}$ and $f_y$.  Plugging these estimates in to Eq.~\eqref{eq:bayes}, we obtain the Bayes plugin classifier:
\begin{align} \label{eq:plugin}
\mh{g}^*_n(\bx) := \argmax_y \mh{f}_{\bx|y}\mh{f}_y.
\end{align}
Under suitable conditions, it is easy to show that this Bayes plugin classifiers performance is asymptotically optimal.  
Formally, we know that:
% \begin{align} \label{eq:pc}
$L_{\mh{g}^*_n} \conv L_{g^*}$.
% \end{align} 


When the parameters, and we want to use a linear approach, we can implement a Bayes plug-in \Lda, which we call Fisher's Discriminant Analysis (\Fld) \cite{Fisher1925a}.  Under the two-class, equal prior, and centered means assumption, we have 
% $\bSig$ and $\bdel$ are unknown, as in real data scenarios, we can use the training samples to estimate them, and plug them in, as in Eq.~\eqref{eq:plugin}:
\begin{align}
\mh{g}^*_n(\bx) := \II\{ \bx\T \mh{\bSig}^{-1} \mh{\bdel} > 0 \},
\end{align}
and $L_{\mh{g}_n}$ is the misclassification rate for an estimated classifier, $\mh{g}_n$.
% This Bayes plugin classifier is called Fisher's Linear Discriminant (FLD; in contrast to \Lda, which uses the true---not estimated---parameters).  
Unfortunately, when $p \gg n$, the estimate of the covariance matrix $\bSig$ will be low-rank, and therefore, not invertible (because an infinite number of solutions all fit equally well).  In such scenarios, we seek alternative methods, even in the \Lda~model.

We would like to prove:
\begin{lem}
$L_{\mh{\Lol}}^d \conv L_*$ for any $\bth \in \bTh'$.
\end{lem}


\begin{lem}
$\PP[L_{\mh{\Pca}}^d \conv L_*]>0$ for any $\bth \in \bTh'$
\end{lem}

\newpage
\section{Finite Sample Theory}

It would be awesome to prove something like:
\begin{thm}
\begin{align*}
\PP[ \mh{L}_{\Lol}^d - \mh{L}_{\Pca}^2  > 0  ] < f(\bth,d,n),
\end{align*}
\end{thm}
which would state that \Lol~is better than \Pca, again, under suitable assumptions.


\begin{thm}
\begin{align*}
\PP[ \bP_{\Pca} \T \bP_*  - \bP_{\Lol} \T \bP_*  > t \norm{\bP_A} \norm{\bP_*} ] < f(t,p,d),
\end{align*}
\end{thm}
which would state that \Lol~is better than \Pca, again, under suitable assumptions.

In terms of distributiosn of the above, it seems that perhaps we could start simple.
Assume for the moment that $\bdel,\bu_1,\ldots,\bu_p \iid \mc{N}(\bmu_p, \bSig_p)$, and let $\bLam=(\bu_1,\ldots,\bu_p)\T$, and $\bSig = \bLam\T \bLam$.  

The reason the above is probabilistic is because it is under certain assumptiosn on the \emph{distributions} of $bdel$, $\bSig$, and $\bA$.   


Perhaps even simpler is to start with specific assumptions about $\bdel$, $\bSig$, and $\bA$. Because \Lda~is rotationally invariant, I believe that we can assert, without loss of generality, that $\bSig=\bS$, where $\bS$ is a diagonal matrix with diagonal entries $\sigma_1,\ldots, \sigma_p$, where all $\sigma_j > 0$.
Now, the optimal projection $\bSig^{-1} \bdel$ is just a simple dot product,  $\bd\T \bdel$, where $\bd=$diag($\bS$)$\in \Real^p$.


For example, letting $\bA=\bU_d$, and letting $\bU_i=e_i$ be the unit vector, with zeros everywhere except a one in the $i^{th}$ position,  we have
\begin{align*}
\bP_A\T \bP_* %&
= \bdel\T \bU_d\T \bU_d \bSig^{-1} \bU_d\T \bU_d \bSig^{-1} \bdel %\\ = 
\bdel\T \bSig_d \bSig^{-1} \bSig_d \bSig^{-1} \bdel %\\&
= \bdel\T \bSig^{-2} \bdel.
\end{align*}


% Consider $\bP_*:=\bP_{\bA_*}$ and $\bP_{\Pca_d} := \bP_{A^{\Pca}_d}$.  
% Now, consider $\alpha_{\Pca} := \angle (\bP_*, \bP_{\Pca_d})$.
% We would like to understand scenarios for which $\angle^*_{\Pca_d}$ is small, and when it is big.




% Note that this angle is a random variable, as it is a function of the sampled data $(\bX_i,Y_i) \iid P$.
So, we want to understand the probability that $\alpha_{\Pca}$ is small under different parameter settings, $\bth \in \bTh$.  

% First note that

% \begin{lem}
% $\bP_A = \bP_{WA}$, when $\bW \T \bW = \bI$.
% \end{lem}


% \begin{proof}
% blah
% \end{proof}

\clearpage
% \bibliography{biblol}
\bibliography{../../../../Other/latex/library.bib}
\bibliographystyle{IEEEtran}


\end{document}
